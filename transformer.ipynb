{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "666e7370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_tokens_with_entities(raw_text: str):\n",
    "    # split the text by spaces only if the space does not occur between square brackets\n",
    "    # we do not want to split \"multi-word\" entity value yet\n",
    "    raw_tokens = re.split(r\"\\s(?![^\\[]*\\])\", raw_text)\n",
    "\n",
    "    # a regex for matching the annotation according to our notation [entity_value](entity_name)\n",
    "    entity_value_pattern = r\"\\[(?P<value>.+?)\\]\\((?P<entity>.+?)\\)\"\n",
    "    entity_value_pattern_compiled = re.compile(entity_value_pattern, flags=re.I|re.M)\n",
    "\n",
    "    tokens_with_entities = []\n",
    "\n",
    "    for raw_token in raw_tokens:\n",
    "        match = entity_value_pattern_compiled.match(raw_token)\n",
    "        if match:\n",
    "            raw_entity_name, raw_entity_value = match.group(\"entity\"), match.group(\"value\")\n",
    "\n",
    "            # we prefix the name of entity differently\n",
    "            # B- indicates beginning of an entity\n",
    "            # I- indicates the token is not a new entity itself but rather a part of existing one\n",
    "            for i, raw_entity_token in enumerate(re.split(\"\\s\", raw_entity_value)):\n",
    "                entity_prefix = \"B\" if i == 0 else \"I\"\n",
    "                entity_name = f\"{entity_prefix}-{raw_entity_name}\"\n",
    "                tokens_with_entities.append((raw_entity_token, entity_name))\n",
    "        else:\n",
    "            tokens_with_entities.append((raw_token, \"O\"))\n",
    "\n",
    "    return tokens_with_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf79bcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'O'), ('come', 'O'), ('from', 'O'), ('Kathmandu', 'B-location'), ('valley,', 'I-location'), ('Nepal', 'B-location')]\n",
      "[('Technos', 'B-brand'), ('39', 'B-display_size'), ('Inch', 'I-display_size'), ('Curved', 'O'), ('Smart', 'O'), ('LED', 'B-display_type'), ('TV', 'O'), ('E39DU2000', 'O'), ('With', 'O'), ('Wallmount', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(get_tokens_with_entities(\"I come from [Kathmandu valley,](location) [Nepal](location)\"))\n",
    "# [('I', 'O'), ('come', 'O'), ('from', 'O'), ('Kathmandu', 'B-location'), ('valley,', 'I-location'), ('Nepal', 'B-location')]\n",
    "\n",
    "print(get_tokens_with_entities(\"[Technos](brand) [39 Inch](display_size) Curved Smart [LED](display_type) TV E39DU2000 With Wallmount\"))\n",
    "# [('Technos', 'B-brand'), ('39', 'B-display_size'), ('Inch', 'I-display_size'), ('Curved', 'O'), ('Smart', 'O'), ('LED', 'B-display_type'), ('TV', 'O'), ('E39DU2000', 'O'), ('With', 'O'), ('Wallmount', 'O')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "531fc686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens           :  ('I', 'have', 'experience', 'in', 'Java', 'python', 'Web', 'development')\n",
      "After subword tokenization:  ['[CLS]', 'i', 'have', 'experience', 'in', 'java', 'python', 'web', 'development', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# note that I purposefully misspell Kathmandu to Kathamanduu\n",
    "sample_input = \"I have experience in [Java](skill) [python](skill) [Web development](skill)\"\n",
    "tokens, entities = list(zip(*get_tokens_with_entities(sample_input)))\n",
    "tokenized_input = tokenizer(tokens, is_split_into_words=True)\n",
    "print(\"Original tokens           : \", tokens)\n",
    "print(\"After subword tokenization: \", tokenizer.convert_ids_to_tokens(tokenized_input['input_ids']))\n",
    "# Original tokens           :  ('I', 'come', 'from', 'Kathmanduu', 'valley,', 'Nepal')\n",
    "# After subword tokenization:  ['[CLS]', 'i', 'come', 'from', 'kathmandu', '##u', 'valley', ',', 'nepal', '[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f05d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataMaker:\n",
    "    def __init__(self, texts):\n",
    "        self.unique_entities = []\n",
    "        self.processed_texts = []\n",
    "\n",
    "        temp_processed_texts = []\n",
    "        for text in texts:\n",
    "            tokens_with_entities = get_tokens_with_entities(text)\n",
    "            for _, ent in tokens_with_entities:\n",
    "                if ent not in self.unique_entities:\n",
    "                    self.unique_entities.append(ent)\n",
    "            temp_processed_texts.append(tokens_with_entities)\n",
    "\n",
    "        self.unique_entities.sort(key=lambda ent: ent if ent != \"O\" else \"\")\n",
    "\n",
    "        for tokens_with_entities in temp_processed_texts:\n",
    "            self.processed_texts.append([(t, self.unique_entities.index(ent)) for t, ent in tokens_with_entities])\n",
    "\n",
    "    @property\n",
    "    def id2label(self):\n",
    "        return dict(enumerate(self.unique_entities))\n",
    "\n",
    "    @property\n",
    "    def label2id(self):\n",
    "        return {v:k for k, v in self.id2label.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        def _process_tokens_for_one_text(id, tokens_with_encoded_entities):\n",
    "            ner_tags = []\n",
    "            tokens = []\n",
    "            for t, ent in tokens_with_encoded_entities:\n",
    "                ner_tags.append(ent)\n",
    "                tokens.append(t)\n",
    "\n",
    "            return {\n",
    "                \"id\": id,\n",
    "                \"ner_tags\": ner_tags,\n",
    "                \"tokens\": tokens\n",
    "            }\n",
    "\n",
    "        tokens_with_encoded_entities = self.processed_texts[idx]\n",
    "        if isinstance(idx, int):\n",
    "            return _process_tokens_for_one_text(idx, tokens_with_encoded_entities)\n",
    "        else:\n",
    "            return [_process_tokens_for_one_text(i+idx.start, tee) for i, tee in enumerate(tokens_with_encoded_entities)]\n",
    "\n",
    "    def as_hf_dataset(self, tokenizer):\n",
    "        from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
    "        def tokenize_and_align_labels(examples):\n",
    "            tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "            labels = []\n",
    "            for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "                word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "                previous_word_idx = None\n",
    "                label_ids = []\n",
    "                for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "                    if word_idx is None:\n",
    "                        label_ids.append(-100)\n",
    "                    elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                        label_ids.append(label[word_idx])\n",
    "                    else:\n",
    "                        label_ids.append(-100)\n",
    "                    previous_word_idx = word_idx\n",
    "                labels.append(label_ids)\n",
    "\n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "            return tokenized_inputs\n",
    "\n",
    "        ids, ner_tags, tokens = [], [], []\n",
    "        for i, pt in enumerate(self.processed_texts):\n",
    "            ids.append(i)\n",
    "            pt_tokens,pt_tags = list(zip(*pt))\n",
    "            ner_tags.append(pt_tags)\n",
    "            tokens.append(pt_tokens)\n",
    "        data = {\n",
    "            \"id\": ids,\n",
    "            \"ner_tags\": ner_tags,\n",
    "            \"tokens\": tokens\n",
    "        }\n",
    "        features = Features({\n",
    "            \"tokens\": Sequence(Value(\"string\")),\n",
    "            \"ner_tags\": Sequence(ClassLabel(names=dm.unique_entities)),\n",
    "            \"id\": Value(\"int32\")\n",
    "        })\n",
    "        ds = Dataset.from_dict(data, features)\n",
    "        tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
    "        return tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8b02ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples = 44\n",
      "[{'id': 0, 'ner_tags': [0, 0, 2, 2, 2], 'tokens': ['', 'Skills:', 'Python', 'Java', 'C++,']}, {'id': 1, 'ner_tags': [0, 2, 2, 2, 4], 'tokens': ['Skills:', 'SQL,', 'Excel,', 'Data', 'Visualization\"']}, {'id': 2, 'ner_tags': [0, 2, 2, 2], 'tokens': ['Skills:', 'Python,', 'Java,', 'C++']}]\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"\"\" Skills: [Python](skill) [Java](skill) [C++,](skill)\n",
    "Skills: [SQL,](skill) [Excel,](skill) [Data Visualization\"](skill)\n",
    "Skills: [Python,](skill) [Java,](skill) [C++](skill)\n",
    "Proficient in [JavaScript,](skill) [HTML,](skill) [CSS\"](skill)\n",
    "Experience with [SQL](skill) and [database management\"](skill)\n",
    "Skilled in [data analysis](skill) using [R](skill) and [Excel\"](skill)\n",
    "Familiar with [machine learning](skill) algorithms: SVM, Random Forest,\n",
    "Proficiency in [MATLAB](skill) and [signal processing](skill)\n",
    "Experience with AWS services: [EC2,](skill) [S3,](skill) [Lambda](skill)\n",
    "Skilled in [web development](skill) using [PHP](skill) [MySQL,](skill) and [Laravel\"](skill)\n",
    "Knowledge of Agile methodologies: Scrum, Kanban\n",
    "Experience in [UI/UX design](skill) using Adobe Creative Suite\n",
    "Proficient with Microsoft Office Suite: [Word,](skill) [Excel,](skill) [PowerPoint](skill)\n",
    "Skilled in front-end development: [HTML](skill) [CSS,](skill) [JavaScript\"](skill)\n",
    "Experience in [mobile app development](skill) using [React Native\"](skill)\n",
    "Proficient in [data visualization](skill) with [Tableau](skill) and [Power BI](skill)\n",
    "Familiarity with version control systems: [Git,](skill) [SVN,] (skill)\n",
    "Experience with backend development frameworks: [Django,](skill) [Flask\"](skill)\n",
    "Skilled in data analysis and visualization using Python libraries: [Pandas,](skill) [Matplotlib,](skill) [Seaborn\"](skill)\n",
    "Proficiency in object-oriented programming languages: [C#](skill), [C++](skill), [Java\"](skill)\n",
    "Experience with cloud platforms: [AWS, ](skill) [Azure,](skill) [Google Cloud\"](skill)\n",
    "Skilled in front-end frameworks: [React, ](skill) [Angular, ](skill) [Vue.js\"](skill)\n",
    "Over [5 years](experience) of experience in [Python](skill) programming and [data analysis\"](skill)\n",
    " Extensive experience in front-end development using HTML, CSS, and JavaScript\n",
    "[10+ years](experience)  of experience in [cloud computing](skill) and managing scalable infrastructure\n",
    " Over [8 years](experience) of experience in responsive design and creating visually appealing interfaces\n",
    " Proven track record of successful branding initiatives with over [6 years](experience) of experience\n",
    "Extensive expertise in visual design principles such as color theory and typography with over [10 years](experience) of experience\n",
    "[5+ years](experience) of experience in social media marketing and running effective campaigns\n",
    " Experienced in content management systems and publishing workflows with a focus on delivering high-quality content\n",
    " Over [5 years](experience)  of experience in software development\n",
    "[10+ years](experience)  of experience in project management\n",
    " Extensive experience in data analysis and reporting\n",
    " [6 years](experience)  of experience in sales and business development\n",
    " Over [8 years](experience) of experience in customer service\n",
    " [3+ years](experience)  of experience in financial analysis\n",
    " Experienced in marketing strategy with [7 years](experience) of experience\n",
    " [10 years](experience)  of experience in healthcare administration\n",
    " Over [6 years](experience) of experience in human resources\n",
    " [4+ years](experience) of experience in graphic design\n",
    " Extensive experience in software testing and quality assurance\n",
    " [8 years](experience) of experience in supply chain management\n",
    " [5+ years](experience) of experience in digital marketing\n",
    "\"\"\"\n",
    "\n",
    "dm = NERDataMaker(raw_text.split(\"\\n\"))\n",
    "print(f\"total examples = {len(dm)}\")\n",
    "print(dm[0:3])\n",
    "\n",
    "# total examples = 35\n",
    "# [{'id': 0, 'ner_tags': [0], 'tokens': ['']}, {'id': 1, 'ner_tags': [2, 3, 0], 'tokens': ['40\"', 'LED', 'TV']}, {'id': 2, 'ner_tags': [0, 2, 0, 0, 3, 0], 'tokens': ['Specifications:', '16″', 'HD', 'READY', 'LED', 'TV.']}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53412fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text=\"\"\" madhu has [Python](skill) and [3 years](experience)\n",
    "John has [JavaScript](skill) and [5 years](experience) Emma has [Data Analysis](skill) and [8 years](experience) \n",
    "David has [C++](skill) and [10 years](experience) Sophia has [Graphic Design](skill) and [2 years](experience)\n",
    "Daniel has [Financial Modeling](skill) and [6 years](experience) Olivia has [Communication](skill) and [4 years](experience)\n",
    "James has [Python](skill) and [7 years](experience) Isabella has [Java](skill) and [9 years](experience) \n",
    "William has [UI/UX Design](skill) and [1 year](experience) Ethan has [Java](skill) and [6 years](experience)  \n",
    "\"\"\"\n",
    "vm = NERDataMaker(val_text.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de821759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d706a15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(dm.unique_entities), id2label=dm.id2label, label2id=dm.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70226dd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "                                              0.0/227.6 kB ? eta -:--:--\n",
      "     ------------                            71.7/227.6 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------              153.6/227.6 kB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------  225.3/227.6 kB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 227.6/227.6 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from torch>=1.6.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from torch>=1.6.0->accelerate) (4.6.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from torch>=1.6.0->accelerate) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->accelerate) (1.2.1)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.20.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "150168d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.28.0\n",
      "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
      "                                              0.0/7.0 MB ? eta -:--:--\n",
      "                                              0.0/7.0 MB 1.9 MB/s eta 0:00:04\n",
      "     -                                        0.2/7.0 MB 2.1 MB/s eta 0:00:04\n",
      "     --                                       0.5/7.0 MB 3.7 MB/s eta 0:00:02\n",
      "     ----                                     0.8/7.0 MB 4.5 MB/s eta 0:00:02\n",
      "     ------                                   1.1/7.0 MB 5.3 MB/s eta 0:00:02\n",
      "     --------                                 1.4/7.0 MB 5.4 MB/s eta 0:00:02\n",
      "     ----------                               1.8/7.0 MB 5.7 MB/s eta 0:00:01\n",
      "     ------------                             2.1/7.0 MB 5.9 MB/s eta 0:00:01\n",
      "     --------------                           2.5/7.0 MB 6.1 MB/s eta 0:00:01\n",
      "     ----------------                         2.8/7.0 MB 6.2 MB/s eta 0:00:01\n",
      "     -----------------                        3.1/7.0 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------                      3.4/7.0 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------                    3.7/7.0 MB 6.3 MB/s eta 0:00:01\n",
      "     -----------------------                  4.0/7.0 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------------------                 4.3/7.0 MB 6.3 MB/s eta 0:00:01\n",
      "     ---------------------------              4.7/7.0 MB 6.4 MB/s eta 0:00:01\n",
      "     -----------------------------            5.1/7.0 MB 6.6 MB/s eta 0:00:01\n",
      "     ------------------------------           5.4/7.0 MB 6.6 MB/s eta 0:00:01\n",
      "     ---------------------------------        5.7/7.0 MB 6.7 MB/s eta 0:00:01\n",
      "     -----------------------------------      6.1/7.0 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------------------------    6.4/7.0 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.8/7.0 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.9/7.0 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.9/7.0 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.9/7.0 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.9/7.0 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.9/7.0 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.9/7.0 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.9/7.0 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.9/7.0 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.9/7.0 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.0/7.0 MB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.28.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akash j\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2022.12.7)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.29.2\n",
      "    Uninstalling transformers-4.29.2:\n",
      "      Successfully uninstalled transformers-4.29.2\n",
      "Successfully installed transformers-4.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad18339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=40,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "train_ds = dm.as_hf_dataset(tokenizer=tokenizer)\n",
    "valid_ds = vm.as_hf_dataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ead9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AKASH J\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/120 03:53 < 00:01, 0.50 it/s, Epoch 39.33/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.398803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.282101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.226805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.169654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.065957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.831624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.738544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.671709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.617956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.567929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.535681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.525872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.509300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.501670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.508223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.494006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.491386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.500087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.508250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.512221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.500219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.478288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.467631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.462106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.461621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.467627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.476591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.478237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.477572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.475793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.472744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.469429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.466365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.464956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.463165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.461622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.460556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.460261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds, # eval on training set! ONLY for DEMO!!\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f78faf60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'experience',\n",
       "  'score': 0.88725305,\n",
       "  'word': '3 years',\n",
       "  'start': 48,\n",
       "  'end': 55},\n",
       " {'entity_group': 'skill',\n",
       "  'score': 0.98414165,\n",
       "  'word': 'java',\n",
       "  'start': 80,\n",
       "  'end': 84},\n",
       " {'entity_group': 'skill',\n",
       "  'score': 0.73956484,\n",
       "  'word': '##script',\n",
       "  'start': 84,\n",
       "  'end': 90},\n",
       " {'entity_group': 'skill',\n",
       "  'score': 0.9879808,\n",
       "  'word': 'react',\n",
       "  'start': 92,\n",
       "  'end': 97},\n",
       " {'entity_group': 'skill',\n",
       "  'score': 0.98215955,\n",
       "  'word': 'node',\n",
       "  'start': 99,\n",
       "  'end': 103},\n",
       " {'entity_group': 'skill',\n",
       "  'score': 0.7793602,\n",
       "  'word': 'j',\n",
       "  'start': 104,\n",
       "  'end': 105},\n",
       " {'entity_group': 'skill',\n",
       "  'score': 0.4953148,\n",
       "  'word': 'computer',\n",
       "  'start': 142,\n",
       "  'end': 150}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")# pass device=0 if using gpu\n",
    "pipe(\"\"\"\n",
    "LOGESH T V\n",
    "\n",
    "+91 96000 54585 | logeshtv.2003@gmail.com | linkedin.com/in/logeshtv/\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "Aspiring full stack developer with an experience in freelancing and internship\n",
    "for the same. Have hands on experience designing, developing and implementing\n",
    "applications and solutions using a range of technologies and programming languages.\n",
    "\n",
    "Seeking to leverage a broad development and hands-on expertise as a full stack\n",
    "\n",
    "developer.\n",
    "PERSONAL INFO\n",
    "\n",
    "Father's Name - Mr .M Vasu\n",
    "Date Of Birth -12/10/2003\n",
    "Address -No. 97, meladhanur village,\n",
    "kodiyum post, tindivanam\n",
    "taluk, villupuram, pin -604207\n",
    "EDUCATION\n",
    "\n",
    "e SRM valliammai engineering college\n",
    "B.Tech - Artificial Intelligence And\n",
    "Data Science\n",
    "MAY 2024 | CHENNAI | cgpa - 8.664\n",
    "\n",
    "e Spring field matriculation Hr. Sec.\n",
    "\n",
    "school\n",
    "MAY 2020 | CHENNAI | mark - 79.16%\n",
    "\n",
    "e Melmaruvathur adhiparasakthi high\n",
    "school\n",
    "MAY 2018 | MELMARUVATHLUR |\n",
    "mark- 83.4%\n",
    "\n",
    "TECHNICAL SKILLS\n",
    "\n",
    "JavaScript, React, Redux, PHP,\n",
    "Express, NodeJS, RESTAPI's\n",
    "\n",
    "FULL STACK\n",
    "DEVELOPER :\n",
    "\n",
    "DATA SCIENTIST: Data analysis and visualisation ,\n",
    "Machine learing , Deep learning,\n",
    "websrapping(BeautifulSoup),\n",
    "openCV\n",
    "\n",
    "DATABASE : SQL and MongoDB\n",
    "\n",
    "ANDROID AND IOS\n",
    "DEVELOPMENT: _ React Native\n",
    "\n",
    "PROGRAMMING python, c+#/c,,R.\n",
    "LANGUAGE :\n",
    "\n",
    "FAMILIAR : Flutter, Flask, DOCKER\n",
    "\n",
    "STRENGTHS\n",
    "\n",
    "e Adaptability\n",
    "\n",
    "e Creativity\n",
    "\n",
    "e Team work\n",
    "\n",
    "e Problem-solving\n",
    "\n",
    "HOBBIES\n",
    "\n",
    "« Writing\n",
    "\n",
    "e Film Direction\n",
    "\n",
    "« Editing\n",
    "LANGUAGES KNOWN\n",
    "\n",
    "e Tamil\n",
    "e English\n",
    "\n",
    " \n",
    "\n",
    "EXPERIENCE\n",
    "\n",
    "Software Developer (FREELANCER) :\n",
    "\n",
    "e freelancer at PeoplePerHour with a rating of 4.5.\n",
    "\n",
    "e worked on more than 10+ big project.\n",
    "\n",
    "e recently worked on a project called myreklam, With this other company\n",
    "can able to post their job and recruit people. which is done using MERN\n",
    "stack. Works like comment section, google map integration, front end\n",
    "works and some more works done.\n",
    "project - https://myreklam.fr/\n",
    "\n",
    "INTERNSHIP - DEVTOWN :\n",
    "\n",
    "* POSITION - FULL(MERN) STACK DEVELOPER.\n",
    "Gained much knowledge while working on real-world problems.\n",
    "During this internship, | completed many projects which will be listed below in\n",
    "the project section.\n",
    "\n",
    "2021-2023\n",
    "\n",
    " \n",
    "\n",
    "JULY 2021- SEPTEMBER 2021\n",
    "\n",
    "certificate -https://cert.devtown.in/verify/Z12NGHV\n",
    "\n",
    "INTERNSHIP - WEB STUDENT :\n",
    "POSITION - FRONT-END DEVELOPER.\n",
    "During the internship, HR promoted me to team leader, and after that, |\n",
    "managed the whole team.\n",
    "\n",
    "while managing | gained knowledge not only about the project and also gained\n",
    "much knowledge about teamwork and leadership.\n",
    "\n",
    "PROJECTS\n",
    "\n",
    "AUGUST 2021- NOVEMBER 21\n",
    "\n",
    "ZOMATO CLONE :\n",
    "* completed the whole web application using react, tailwind for front end.\n",
    "* express and nodejs for server and backend, mongoDB for database.\n",
    "* amazon S3 bucket for image and video file pipeline. And docker is also used\n",
    "\n",
    "PROJECT - https://github.com/logeshloki585/Zomato-Master-Repo\n",
    "IMDB MOVIE ANALYSIS THROUGH WEB SCRAPPING :\n",
    "* project is done using python libraries such as numpy, pandas. And matplotlib is\n",
    "used for data visualization .\n",
    "e For web scrapping library called beautifulSoup is used.\n",
    "PROJECT - https://colab research. google.com/drive/1uTMf7wZIIf1dpyVRDxq7ZAdjmfa2kvnN?\n",
    "usp=sharing\n",
    "\n",
    "INTELLIGENT SURVEILLANCE SYSTEM :\n",
    "\n",
    "e Project is build using the python library Opencv .\n",
    "\n",
    "e And implemented the face detection in the web page using the python frame\n",
    "work tkinter and flask.\n",
    "\n",
    "* And integrated with QR code scanner for fetching training data from the\n",
    "database to find the person.\n",
    "\n",
    "BOOKMYSHOW, NETFLIX AND YOUTUBE CLONE - (FRONT END) :\n",
    "\n",
    "e front end is build using react .\n",
    "e And movie data are fetched from the moviesdb api using axios\n",
    "* projects are mentioned in porfolio\n",
    "\n",
    "LINKS\n",
    "\n",
    "   \n",
    "\n",
    "LINKEDIN - linkedin.com/in/logeshtv/\n",
    "GITHUB - —_github.com/logeshloki585\n",
    "PEOPLEPERHOUR - peopleperhour.com/logesh-t_v\n",
    "\n",
    " \n",
    "\n",
    "PORFOLIO - logeshloki585.vercel.app/\n",
    "\"\"\")\n",
    "# pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "data analyst: [Python, R, SQL, Tableau, Machine Learning],\n",
    "    full stack developer: [Python, JavaScript, React, SQL, Git],\n",
    "    front end developer: [HTML, CSS, JavaScript, React, UI/UX],\n",
    "    data scientist: [Python, R, SQL, Machine Learning, Statistics],\n",
    "    data engineer: [Python, SQL, ETL, Big Data, Data Modeling],\n",
    "    software engineer: [Java, C++, Python, JavaScript, Git],\n",
    "    business analyst: [Data Analysis, Business Intelligence, SQL, Tableau],\n",
    "    product manager: [Product Development, Agile Methodology, Market Research, Strategy],\n",
    "    UI/UX designer: [User Research, Wireframing, Prototyping, Usability Testing],\n",
    "    project manager: [Project Planning, Risk Management, Team Leadership, Communication],\n",
    "    network administrator: [Network Troubleshooting, LAN/WAN Configuration, Network Security, Cisco Routing and Switching],\n",
    "    systems administrator: [Server Administration, Active Directory Management, Virtualization, Backup and Recovery],\n",
    "    cybersecurity analyst: [Threat Detection and Response, Vulnerability Assessment, Incident Handling and Forensics, Security Frameworks],\n",
    "    cloud architect: [Cloud Platforms, Infrastructure as Code, Microservices Architecture, Containerization],\n",
    "    data architect: [Data Modeling, Database Design, Data Integration, Data Governance],\n",
    "    machine learning engineer: [Machine Learning Algorithms, Deep Learning, Data Preprocessing, Model Evaluation],\n",
    "    devops engineer: [Continuous Integration/Deployment, Container Orchestration, Infrastructure Automation, Monitoring and Logging],\n",
    "    software tester: [Test Planning, Test Automation, Defect Tracking, Regression Testing]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
